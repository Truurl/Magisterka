
\chapter{Przegląd literatury}
    \justify  
    \section{Domain adaptation}
         W wielu przypadkach uczenie maszynowe zakłada, że zbióry danych treningowe i testowe (również rozumiane jako realne środowisko) pochodzą z identycznych rozkładów lub posiadają podobny rozkład cech. Przyjmując takie założenia można spodziewać się, że poprawnie wyszkolony model nie powinien tracić na jakości, gdy zmienni się jego środowisko pracy - przenosząc go ze środowisko szkoleniowego do ptawdziwego życia. Jednak praktyka pokazuję, że to założenie często nie znajduje potwierdzenia a jakość działania modelu obniża się przy zmianach środowiska pracy. Jest to spowodowane różnicą w dystrybucji zbiorów danych (testowych i treningowych) oraz realnym środowiskiem. Zróżnicowane dystrybucje danych wynikają z trudności w pozyskiwaniu danych o jednolitych właściwościach, co jest dodatkowo skomplikowane przez powszechne stosowanie publicznych zbiorów danych, a tworzenie nowych jest kosztowne i czasochłonne\cite{domain_adaptation_review}.
        \par
        Problem \textit{domain adaptataion} dotyczy sytuacji, w której zbiory danych treningowe i testowe, określane odpowiednio jako \textit{source domain} i \textit{target domain}, charakteryzują się różnicą w rozkładach, znaną jako \textit{domain gap}. Różnica ta prowadzi do pogorszenia jakości działania modelu, zjawiska nazywanego \textit{domain shift}. W celu przeciwdziałania temu problemowi, rozwijana jest dziedzina \textit{domain adaptation}, która za pomocą specyficznych technik adaptacyjnych umożliwia modelom uczenia maszynowego efektywną pracę w różnych, realnych zastosowaniach. Adaptacja domeny, będąc formą \textit{transfer learningu}, ma na celu poprawę działania modelu w sytuacji ograniczonej dostępności danych z \textit{target domain} poprzez wykorzystanie wiedzy z \textit{source domain}. W odróżnieniu od \textit{transfer learningu}, gdzie możliwa jest zmiana zadania modelu, w \textit{domain adaptation} zadanie pozostaje niezmienione, a różnice występują między domenami.
        \par 
        Rozwiązaniem powyższego problemu jest nowa dziedzina związana z uczeniem mawszynowynm \textit{Domain adaptation}. Techniki adaptacji domeny pozwalają modelom uczenia mszynowego na odowiednią naukę taką, aby mogły sobie świetnie radzić w realnych zastotowaniach czy też w zbioprach testowych. \textit{Doman adaptation} jest to w pewnym rozumiemu \textit{transfer learning}\cite{transfer_learning}. Obie dziedziny starają się
        rozwiązać ten sam problem ale na dwa różne sposoby. Wspólnym mianownikiem obu przypadków jest to, że starają się poprawić jakość działania modelu mając nie wystarczającą ilość danych (rozumianą jako \textit{target domain}) poprzez wykorzystanie wiedzy wydobytej z innego zbioru danych (\textit{source domain}). Przypadki \textit{transfer learning} to sytuacje gdzie zadanie modelu jest zmieniane, ale domena może być pozostawiona lub może się zmienić pomiędzy \textit{source} i \textit{target}. Natomiast w sytuacjach \textit{domain adaptation} zadnaie dla modelu uczenia maszynowego się nie zmienia, za to \textit{source domain} i \textit{target domain} rożnią się wzgledem siebie.
        \par
        Istnieją jeszze inne metody radzenia z wyżej opisanym problemem. \textit{Semi-supervised classification}\cite{semi-sup-learning} jest jedną z takich metod. Polega ona, na tworzeniu modeli uczenia maszynowego, którą korzystają z danych z etykietami oraz bez etykiet (ang. label), przy czym zakłada się, że oba zbiory danych pochodźa z tego samego rozkładu prawpodobieństwa. Kolejną metodą jest \textit{Multi task learning}\cite{multitask-learning}, które stara się polepszyć generalizację pomiędzy wioeloma spokrewinonymi zadaniami poprzez jednoczesne szkolenie. Można, załozyć że podbne zadania będą musiały korzystać ze wspołnych informacji, Model uczony w taki sposób, zmuszony jest do dokładnego poznania struktury danych oraz współdzielenia reprezentacji danych pomiędzy różnymi zadaniami. \textit{Multi task learning} i \textit{Transer learning} stosują podobne metody m.in wspóldzielenie parametrów, transformacja cech, jednakże \textit{transer learning} jest wykorzystywany do poprawy działania modelu poprzez najpierw uczenia go na \textit{source domain} a później transferu wydobytej wiedzy do \textit{target domain}. \textit{Multi task learning} skupia się na poprawy jakości wielu spokrewinoch zadań poprzez wspólne uczenie.
        \par
        Kolejną metodą jest \textit{multi-view learning}, które korzysta z wielu różnych zbiorówch danych, które różnią się cechami. Przykładami takich połączeń mogą być: dźwięk + film, obraz + tekst i tekst + tekst (przykładem może być strona internetowa, posiada on kod źródłowy i informacje czesto w postaci tekstu). Taki typ uczenia maszynowego uzasadnia się tym, że mieszanie zbiorów poniekąd tworzy pewien komplementarny opis danych co poprawia działanie modelu. 
        \par
        Ostanią motodę podobną do \textit{domain adaptation} jest \textit{domain generalization}\cite{domain-generalization}, czyli uczenie maszynowe korzystające z wielu zbiorów danych treningowych (\textit{source domain}) co zwiększa liczbę oznakowanych danych, przez co model powinien lepiej generalizowac dane wejściowe. Jest to bardzo podobne podejście do \textit{domain adaptation}, ale z tą różnicą, że \textit{target domain} nie jest znane podczas procesu treningu. Techniki \textit{domain adaptation} wymagają znania \textit{target domain}, aby można było skompensować \textit{domain shift}.
        
        \subsection{Definicja domain adaptation}
            Rozważając problematykę \textit{domain adaptation}, zbiory danych, czyli domeny (ang. domain) uznaje się za struktury złożone z trzech elementów:
            \begin{equation*}
                \mathcal{D} = \{\mathcal{X},\mathcal{Y},p(x,y)\}
            \end{equation*}
            gdzie:
            
            \begin{tabular}{rl}
                $\mathcal{X}$ -& przestrzeń cech danego zbioru danych \\
                $\mathcal{Y}$ -& przestrzeń etykiet danego zbioru danych \\
                $p(x,y)$ -& rozkład prawpodobieństwa przestrzeni cech i etykiet zbiorów
            \end{tabular}
            \linebreak
            
            Sama przestrzeń $\mathcal{X}$ jest podzbiorem d wymiarowej przestrzeni, $\mathcal{X} \subset \mathbb{R}^{d}$, natomiast przestrzeń $\mathcal{Y}$ to przestrzeń binarna $\{ -1, 1\}$ lub wieloklasowa przestrzeń $\{1,2\cdots K \}$, gdzie $K$ to liczba klas w danym zbiorze danych. Przez $p(x,y)$ to połączony rozkład prawpodobieńst przestrzeni par cecha-etykieta $\mathcal{X} \times \mathcal{Y}$. Samo $p(x,y)$ może zostać rozłożonę na następujące równanie:
            \begin{equation*}
                p(x,y) = p(x)p(x|y) = p(y)(p(y|x))
            \end{equation*}
            \par
            Wykonuwując próbę losową na zbirze treningwowym $\mathcal{S}$ oraz testowym $\mathcal{T}$ otzrymujemy pary cecha-etykieta
            $\{(x_i, y_i)\}^{n}_{i=1}$, gdziie \textit{n} oznacza liczbę próbek ze zbioru \textit{source domain}. Analogicznie dla \textit{target domain} można wydedukować $\{(z_i, y_i)\}^{m}_{i=1}$. W przypadku, gdy \textit{source domain} i \textit{target domain} są do sibie zbliżone tzn. mają podobne cechy i reprezentują podobne obiekty, ale posiadają różne rozkłady prawpodobieństw to można założyć, że ekstrapolując wiedźę wydobytą z jednego zbioru danych (\textit{source domain}, otrzymamy pogorszoną jakość działania modelu uczenia maszynowego na zbiorze \textit{target domain}). 
        \subsection{Kategorie Domain Adaptation}
        
    \pagebreak 
    \section{Domain gap}
    \pagebreak
    \section{Przegląd technik domain adaptation}
    \pagebreak
    \section{Generacyjne sieci neuronwoe}
        Rozwój uczenia maszynowego pozwolił na odkrycie modeli generacyjnych, które zrewolucjonizowały sposób, w jaki maszyny mogą naśladować ludzką kreatywność. Wśród nich, szczególne miejsce zajmuje GAN (Generative Adversarial Network), który został zaprezentowany przez  Iana Goodfellowa i współpracowników w 2014 roku\cite{goodfellow}. Sukces GAN opiera się na ich unikatowej architekturze, składającej się z dwóch sieci uczących się w procesie rywalizacji: generatywnej, która tworzy nowe dane, i dyskryminacyjnej, która ocenia ich autentyczność.\par
        Celem tej sekcji jest dogłębne przybliżenie modeli GAN,, procesu ich treningu i implementacji. Omówione zostaną kluczowe koncepcje stojące za GAN, architektura tych sieci, a także różnorodne warianty, które powstały na przestrzeni lat. Dodatkowo, rozdział ten rzuci światło na wyzwania związane z treningiem GAN oraz strategie ich przeciwdziałania, zapewniając czytelnikowi kompleksowe zrozumienie tej przełomowej technologii w generatywnym uczeniu maszynowym.
        \subsection{GAN (Generative Adversarial Network)}
            Modele GAN (Generative Adversarial Network) to jeden z typów sieci neuronowyc służących do generowania danych. Podstawowym założeniem GAN to dwa modele będące wobec siebie wrogie. Jednym z modeli jest generator G oraz dyskriminator D. Zadaniem generatora jest tworzenie nowych danych, natomiast zadaniem dyskriminatora jest prównywanie syntetycznych danych z docelowymi - zazwyczaj są one danymi rzeczywistymi. Szkolenie modeli typu GAN polega na grze o sumie zerowej\cite{gan-intro}. W każdym kroku sumaryczna poprawa modeli D i G jest zerowa, gdy jeden z modeli polepsza się, drugi traci. 
            \subsubsection{Architektura GAN}
            \subsubsection{Problemy modeli typu GAN}
            \subsubsection{Wariacje GAN}
    \pagebreak
